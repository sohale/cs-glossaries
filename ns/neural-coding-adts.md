A cute (toy-like) document about various ways Neural Coding helps compuation:

Genius ideas designed by nature/evolution that makes our hard problems easier and tractable. In the sense of ADTs.

Things that moved me.

### Insect circuits Spatial Integration of location (Webb)
### LMC encoding for c e (Hadi)
### Time of first-spike coding (Thorpe)
### (Event-based) y-discritisation coding (Panzeri's collaborator)
### Phase coding of location? (How does it help?)
### Phase coding for bursts (Montemurro)
### Clasical Pouget Population coding
### Clasical Zhang Population coding
Used for head direction, etc
### (Machens)
Machens clash of threshold: mask away
### Adrian and Integrate and Fire
### (classic ANN:) simple thresholds
#### (classic ANN:) tanh
#### (classic ANN:) tanh as probability
#### (classic ANN:) ReLU

### Future
* Limbic-PFC interactions (mask logic)
* Disinhibition in BG
* VTA encodes prediction error of value (acuumulating reward)
* RNN, replay: episodic
* LFP as population activityy
* LFP phase?
* Using 0,0,0,1,0,0,0 response (a kind of sparse) in ANN, classifiers, etc
* Samples: Framing sampling hypothesis as a coding? (Also Boltzmann Machine, Also Boltzmann's encoding of energy as index of probablity, similar to Quantum)
* PP: (coding of, representing, exchanging) errors (deltas)
* ( Weights as parameters of ANN: Simple Gradient)
* (humans trick of surrpogate ... for SSNs: approximating with simplified models!)
* Dopmaine-ACh (gating Dopamine using ACh)
* SNc initiating action and constaining its ttart to a DA drive.
* Representations of uncertainty (and confidence, etc: using smaple, using population coding, etc)
* Can read-outs be coding? vector average, averaging / pooling, winner-take-call
* Hopfield: encoding state in attern of (1,0)^N (Also Botlzmann)


