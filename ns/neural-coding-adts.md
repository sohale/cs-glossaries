# A tour of Neural codes

Paradigm used here: Neural coding + Learning

A cute (toy-like) document about various ways Neural Coding helps compuation:

Genius ideas designed by nature/evolution that makes our hard problems easier and tractable. In the sense of ADTs.

Things that moved me.

They are usualley good for:
* Generalisations (low-sample: pverty)
* How NS teaches AI
* Response speed

### Insect circuits Spatial Integration of location (Webb)
### LMC encoding for c e (Hadi)
### Time of first-spike coding (Thorpe)
### (Event-based) y-discritisation coding (Panzeri's collaborator: chiara bartolozzi)
doi: 10.1162/NECO_a_00703
### Phase coding of location? (How does it help?)
### Phase coding for bursts (Montemurro)
### Clasical Pouget Population coding
### Clasical Zhang Population coding
Used for head direction, etc
### (Machens)
Machens clash of threshold: mask away
### Adrian and Integrate and Fire
### (classic ANN:) simple thresholds
#### (classic ANN:) tanh
#### (classic ANN:) tanh as probability
#### (classic ANN:) ReLU
### Coding Index in Hippocampus (Temporal Hippocampal Formation) (Davachi)

### Future
* Limbic-PFC interactions (mask logic)
* Disinhibition in BG
* VTA encodes prediction error of value (acuumulating reward)
* RNN, replay: episodic
* LFP as population activityy
* LFP phase?
* Using 0,0,0,1,0,0,0 response (a kind of sparse) in ANN, classifiers, etc
* Samples: Framing sampling hypothesis as a coding? (Also Boltzmann Machine, Also Boltzmann's encoding of energy as index of probablity, similar to Quantum)
* PP: (coding of, representing, exchanging) errors (deltas)
* ( Weights as parameters of ANN: Simple Gradient)
* (humans trick of surrpogate ... for SSNs: approximating with simplified models!)
* Dopmaine-ACh (gating Dopamine using ACh)
* SNc initiating action and constaining its ttart to a DA drive.
* Representations of uncertainty (and confidence, etc: using smaple, using population coding, etc)
* Can read-outs be coding? vector average, averaging / pooling, winner-take-call
* Hopfield: encoding state in attern of (1,0)^N (Also Botlzmann)
* Tuning curve, reponse profile. (Also: in general: distributions)
* (unsure: PRC: Phase resetting curve?)
* Generating: encoding the un-unfolded (folded) and unfold it when necessary (in low-level)
* The ...
* (grandmother cell?)
* (concept cell?) (aslso see: Index cell)
* Probabilities in OFC versus some kind of probability in mVLPFC
* Probability in Amygdala and Insular cortex
* (forgot)
* (amygdalae for (two) takes)
* Risk i Amygdala (alarm, vigilance, etc)
* RL: representing value by cacheing it. (version 1: sum_ future reward, as mediating-virtual value that is attempted by RL system) 
* Motor recruitment: (dunamic run-time (!) involvement)
* (Spikes in Muscle)
* (Force in muscles: and sensing it)
* All sensory (and sensory feedback)
* Fast adapting and slow adapting in touch
* Citi: on-going multiple spikes + censored-Inverse-Gausssian-distributin (very low delay)
* Any encoding of Markov shadow (simplified: for generalisation)
* Auto-encoder
* Variational (a way to model probability: bu tin fact a ways of modelling: by separating things)
* Bagging/boosting? (and see distribution)
* Distributed: for various places of space of states
* Distributed: Population (various ways Clark describes)
* (Distributed: In teh sense of early ANN papers)
* (Sparsity: anything: Sparsity in learning. Sparsity in activity) (also see energy-efficient)
* Energy-efficiency consequences (and constraitns: how it helps and how it causes problem)
*
* (bottleneck method)
* Specialised areas that integrate (Amygdala, )
* Spatial frequency
* Fourier refpresentation in vision (Gabor, Phase, Hadi's phase, Spatial frequency)
* Audio? We will see.
* 5-dimentional trick of V1 (both location and other features)
* Location as a mere feature
* AI: Transformer: encoding attention as code
* Idea of representing Prediction (PP view, "trying" to predict) but also error-deltas. (see delay)
* (Efference copy?)
