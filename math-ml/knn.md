# kNN: k-nearest neighbors algorithm
Describe the KNN

### Context
It's a classifier:

Given: `DataSet` = set of $(X_,y_i) \in R^d\times C$, where $C=\\{1,2\\}$ is the set of labels/colours . ðŸ”‘

Find $f(X)$ for $f: R^d\rightarrow C$ for any âˆ€ $X\in R^d$

$X$ can be points not in the `DataSet` (interpoltion and extrapolation).

### The main algorithm
[â‹¯]


The main technical technic in proof is this: "as the amount of data approaches infinity" (keeps the distribution the same, distances decrease).

## Related:

###  Bayes error rate
[Bayes error rate](https://en.wikipedia.org/wiki/Bayes_error_rate) := the minimum achievable error rate given the distribution of the data. ðŸŒªðŸ•³

### k-d trees
https://en.wikipedia.org/wiki/K-d_tree

[â‹¯]

### Legend
* ðŸ”‘: key point, was missing. An insight. unblock.
* ðŸŒª: missing concept in my kowledge (hole ðŸ•³). A Lack.
* [â‹¯] to be filled in (but not really missing in mind. not a Lack)
